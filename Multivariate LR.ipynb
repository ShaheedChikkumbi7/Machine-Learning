{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1bvJ8odsGPBmh-2nxUkxHIn2hwW64pXur","authorship_tag":"ABX9TyOzqr9ktkD9luhxZds9PkK8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"T4w8mzmIcp6S","executionInfo":{"status":"ok","timestamp":1697427280914,"user_tz":-330,"elapsed":1193,"user":{"displayName":"Shaheed Chiikumbi","userId":"04356927506517514009"}},"outputId":"a69788e8-2317-488f-b2d8-3524deefe30a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["   x1  x2    y\n","0   5   9  100\n","1   6   3  200\n","2   2   7  300\n","3   4   5  400\n","4   8   7  500\n","5   9   5  600\n","6   6   4  700\n","[[5 9]\n"," [6 3]\n"," [2 7]\n"," [4 5]\n"," [8 7]\n"," [9 5]\n"," [6 4]]\n","[[100]\n"," [200]\n"," [300]\n"," [400]\n"," [500]\n"," [600]\n"," [700]]\n","[5.71428571 5.71428571]\n","[2.18529408 1.90595201]\n","[[-0.32686023  1.72392288]\n"," [ 0.13074409 -1.42411021]\n"," [-1.69967317  0.67457852]\n"," [-0.78446454 -0.37476584]\n"," [ 1.04595272  0.67457852]\n"," [ 1.50355704 -0.37476584]\n"," [ 0.13074409 -0.89943803]]\n","[[ 1.         -0.32686023  1.72392288]\n"," [ 1.          0.13074409 -1.42411021]\n"," [ 1.         -1.69967317  0.67457852]\n"," [ 1.         -0.78446454 -0.37476584]\n"," [ 1.          1.04595272  0.67457852]\n"," [ 1.          1.50355704 -0.37476584]\n"," [ 1.          0.13074409 -0.89943803]]\n","[[0.]\n"," [0.]\n"," [0.]]\n","optimized weights: [[399.99999933]\n"," [ 83.72833373]\n"," [-63.57672945]]\n","Optimized coefficients (theta): [[ 83.72833373]\n"," [-63.57672945]]\n","Optimized intercept: [399.99999933]\n","   X1  X2\n","0   4   5\n","1   6   4\n","2   7   8\n","3   5   3\n","4   9   6\n","5   2   7\n","6   3   5\n","[[4 5]\n"," [6 4]\n"," [7 8]\n"," [5 3]\n"," [9 6]\n"," [2 7]\n"," [3 5]]\n","[[-0.51214752 -0.26940795]\n"," [ 0.38411064 -0.89802651]\n"," [ 0.83223972  1.61644772]\n"," [-0.06401844 -1.52664507]\n"," [ 1.72849788  0.3592106 ]\n"," [-1.40840568  0.98782916]\n"," [-0.9602766  -0.26940795]]\n","[[ 1.         -0.51214752 -0.26940795]\n"," [ 1.          0.38411064 -0.89802651]\n"," [ 1.          0.83223972  1.61644772]\n"," [ 1.         -0.06401844 -1.52664507]\n"," [ 1.          1.72849788  0.3592106 ]\n"," [ 1.         -1.40840568  0.98782916]\n"," [ 1.         -0.9602766  -0.26940795]]\n","Predicted value for new input: [[374.24681742]\n"," [489.25453164]\n"," [366.91358505]\n"," [491.69894243]\n"," [521.8868112 ]\n"," [219.27359127]\n"," [336.72571628]]\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","data=pd.read_csv('/content/drive/MyDrive/Book2.csv')\n","print(data)\n","\n","x=np.array(data[['x1','x2']])\n","y=np.array(data[['y']])\n","print(x)\n","print(y)\n","\n","x_mean=np.mean(x,axis=0)\n","x_std=np.std(x,axis=0)\n","\n","print(x_mean)\n","print(x_std)\n","\n","x=(x-x_mean)/x_std\n","print(x)\n","\n","x_b=np.c_[np.ones((len(x),1)),x]\n","print(x_b)\n","\n","theta=np.zeros((x_b.shape[1],1))\n","print(theta)\n","\n","alpha=0.01\n","iterations=1000\n","\n","for i in range(iterations):\n","  y_pred=x_b.dot(theta)\n","  error=y_pred-y\n","\n","  gradient=(2/len(y))*x_b.T.dot(error)\n","  theta=theta-(alpha*gradient)\n","\n","print(\"optimized weights:\",theta)\n","\n","print(\"Optimized coefficients (theta):\", theta[1:])\n","print(\"Optimized intercept:\", theta[0])\n","\n","data_test=pd.read_csv('/content/drive/MyDrive/book_test.csv')\n","print(data_test)\n","x_test=np.array(data_test[['X1','X2']])\n","print(x_test)\n","\n","x_test=(x_test-np.mean(x_test,axis=0))/np.std(x_test,axis=0)\n","print(x_test)\n","\n","x_test_b=np.c_[np.ones((len(x_test),1)),x_test]\n","print(x_test_b)\n","\n","y_predicted=x_test_b.dot(theta)\n","print(\"Predicted value for new input:\", y_predicted)"]},{"cell_type":"markdown","source":["Import Libraries:\n","\n","Import the necessary libraries: Pandas, NumPy, and Matplotlib for data manipulation, numerical operations, and plotting, respectively.\n","Read Data:\n","\n","Read a CSV file named 'Book2.csv' located at '/content/drive/MyDrive/Task_1/' into a Pandas DataFrame and store it in the variable data.\n","Data Preparation:\n","\n","Extract the 'x1' and 'x2' columns from the DataFrame and store them as NumPy arrays in the variable x.\n","Extract the 'y' column from the DataFrame and store it as a NumPy array in the variable y.\n","Feature Scaling:\n","\n","Compute the mean and standard deviation of each feature in x.\n","Standardize the features by subtracting the mean and dividing by the standard deviation. This step is essential for proper gradient descent convergence.\n","Add a Bias Term:\n","\n","Add a column of ones to the x array, creating a design matrix x_b. This column represents the bias or intercept term.\n","Initialize Model Parameters:\n","\n","Initialize the parameter vector theta with zeros. The length of theta is determined by the number of features plus one (due to the added bias term).\n","Set Hyperparameters:\n","\n","Define the learning rate (alpha) and the number of iterations (iterations) for gradient descent.\n","Gradient Descent:\n","\n","Perform gradient descent to optimize the model parameters (theta) by minimizing the mean squared error.\n","In a loop that runs for iterations:\n","Compute predictions (y_pred) by multiplying the design matrix x_b by the current parameter vector theta.\n","Calculate the error between predictions and actual values (the difference between y_pred and y).\n","Compute the gradient of the cost function with respect to theta and update theta using the gradient and the learning rate.\n","Display the Optimized Parameters:\n","\n","Print the optimized weights (coefficients) in theta, excluding the intercept.\n","Print the optimized intercept.\n","Testing the Model:\n","\n","Read another CSV file named 'book_test.csv' into a Pandas DataFrame and store it in the variable data_test.\n","Extract the 'X1' and 'X2' columns from the test data and standardize them similar to the training data.\n","Add a bias term to the test data to create x_test_b.\n","Calculate predictions for the test data by multiplying x_test_b by the optimized theta.\n","Print Predictions:\n","\n","Print the predicted values for the new input data.\n","This code essentially performs linear regression, where it learns a linear model to predict the target variable y based on the input features x1 and x2. The model is trained using gradient descent to find the optimal coefficients for the linear equation, and it is then used to make predictions on new input data. The code assumes that both the training and test data are stored in CSV files and follows best practices for preprocessing and scaling the data before training and testing the model.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"_1H4ekxEhfC3"}}]}